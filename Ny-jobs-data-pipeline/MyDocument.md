
# Solution

## Notebooks
 
 - There are four Notebokks for Solution, Access the notebooks in order of 0 to 3.

 ![jupyter-notebook](https://github.com/murtazanaiyar/project1/blob/main/pictures/jupyter-noteboos.png)
    
## Test Cases
  
  - Test cases can be triggerred using make file :
  
![jupyter-notebook](https://github.com/murtazanaiyar/project1/blob/main/pictures/test-cases.png)
   
## Pipeline : Workflow Management and Sheduling using Airflow
  
  - Pipeline's workflow is orchestrated usinhg Apache Airflow.
  
![jupyter-notebook](https://github.com/murtazanaiyar/project1/blob/main/pictures/Containers.png)

![jupyter-notebook](https://github.com/murtazanaiyar/project1/blob/main/pictures/dags.png)

# Challanges

- Working with spark using docker has been challaneging since, it requires long hours to develop the solution.
- At times notebooks stopped responding. So resource management has been challenging.

# Learnings
- Data give us a brief insight about how requirement of skills has changed over time market.
- In a less time how to develop and package a complete Data Engineeirng solution, Which requires skill
of Data Science, Data Engineering and Devops as well.

# Assumptions
- My Airflow solution for workflow management can be more optimised, Assuming when we develop a realtime
project we have better resource capablity.
- Airflow Dag from my solution is more oriented towards demonstarting my skills of pipelineing and software engineering
than the processing of actual data using pipeline. 
